{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "acc = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>User ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15624510</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15810944</th>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15668575</th>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603246</th>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15804002</th>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Gender  Age  EstimatedSalary  Purchased\n",
       "User ID                                          \n",
       "15624510    Male   19            19000          0\n",
       "15810944    Male   35            20000          0\n",
       "15668575  Female   26            43000          0\n",
       "15603246  Female   27            57000          0\n",
       "15804002    Male   19            76000          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv', index_col=0)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, [1, 2]].values\n",
    "y = dataset.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state = 0)\n",
    "lr.fit(X_train, y_train)\n",
    "models.append('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model on disk\n",
    "pk.dump(lr, open('logistic_regression.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.06169269, 1.10338288]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.92421803])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8835313 , 0.82871699, 0.8004837 , 0.90739447, 0.89926493,\n",
       "       0.99169199, 0.98410063, 0.26459889, 0.99384437, 0.4907332 ,\n",
       "       0.96238257, 0.96947917, 0.83490307, 0.62448827, 0.98369807,\n",
       "       0.65886488, 0.7144286 , 0.98708287, 0.01150228, 0.95626217,\n",
       "       0.90763174, 0.03668135, 0.71749074, 0.11706887, 0.99524108,\n",
       "       0.02710673, 0.91943183, 0.92088349, 0.80168341, 0.83888382,\n",
       "       0.97879075, 0.70119816, 0.06769477, 0.84353432, 0.98491826,\n",
       "       0.99630128, 0.97919709, 0.93195984, 0.97118522, 0.4448538 ,\n",
       "       0.93130023, 0.71096146, 0.93904903, 0.95886141, 0.19345382,\n",
       "       0.9725232 , 0.69445268, 0.07165789, 0.99068214, 0.1393124 ,\n",
       "       0.00989222, 0.96057081, 0.87708921, 0.57783027, 0.01932703,\n",
       "       0.67330527, 0.91360278, 0.95550802, 0.50860878, 0.99631168,\n",
       "       0.97865378, 0.06541544, 0.99028301, 0.60285451, 0.99802191,\n",
       "       0.01850645, 0.96035612, 0.9725232 , 0.77974568, 0.5299776 ,\n",
       "       0.40192565, 0.79006863, 0.98819632, 0.73944348, 0.92922501,\n",
       "       0.9904    , 0.45813315, 0.70021207, 0.27404394, 0.11061585,\n",
       "       0.00401366, 0.02260363, 0.98627946, 0.98908355, 0.10687816,\n",
       "       0.45020289, 0.59108435, 0.00511557, 0.55751522, 0.65568733,\n",
       "       0.57737278, 0.21579253, 0.99101483, 0.99162973, 0.96601697,\n",
       "       0.91300665, 0.98556125, 0.51965484, 0.14870417, 0.27535436])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = lr.predict_proba(X_test)\n",
    "probs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model to predict\n",
    "loaded_model_lr = pk.load(open('logistic_regression.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                    random_state = 0)\n",
    "dt.fit(X_train, y_train)\n",
    "models.append('Decision Trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel = 'rbf', random_state = 0)\n",
    "svc.fit(X_train, y_train)\n",
    "models.append('SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 350, criterion = 'entropy', \n",
    "                                    random_state = 0)\n",
    "rf.fit(X_train, y_train)\n",
    "models.append('Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting AdaBoost Classification to the Training set\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adb = AdaBoostClassifier(base_estimator=dt, n_estimators=50, \n",
    "                         algorithm='SAMME.R', random_state=40)\n",
    "adb.fit(X_train, y_train)\n",
    "models.append('AdaBoost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Voting Classifier Classification to the Training set\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vc = VotingClassifier(estimators=[('Logistic Regression',lr),\n",
    "                                   ('SVM',svc),\n",
    "                                   ('Decision Tree',dt),\n",
    "                                   ('Random Forest',rf),\n",
    "                                   ('AdaBoost',adb)], \n",
    "                       voting='hard')\n",
    "                       #flatten_transform=True)\n",
    "vc.fit(X_train, y_train)\n",
    "models.append('Average Ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('Logistic Regression',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=0, solver='warn',\n",
       "                                                 tol=0.0001, verbose=0,\n",
       "                                                 warm_start=False)),\n",
       "                             ('SVM',\n",
       "                              SVC(C=1.0, cache_size=200, class_weight=None,\n",
       "                                  coef0=0.0, decisi...\n",
       "                                                                                       criterion='entropy',\n",
       "                                                                                       max_depth=None,\n",
       "                                                                                       max_features=None,\n",
       "                                                                                       max_leaf_nodes=None,\n",
       "                                                                                       min_impurity_decrease=0.0,\n",
       "                                                                                       min_impurity_split=None,\n",
       "                                                                                       min_samples_leaf=1,\n",
       "                                                                                       min_samples_split=2,\n",
       "                                                                                       min_weight_fraction_leaf=0.0,\n",
       "                                                                                       presort=False,\n",
       "                                                                                       random_state=0,\n",
       "                                                                                       splitter='best'),\n",
       "                                                 learning_rate=1.0,\n",
       "                                                 n_estimators=50,\n",
       "                                                 random_state=40))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=[1, 5, 2, 4, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Voting Classifier Classification to the Training set\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "vc2 = VotingClassifier(estimators=[('Logistic Regression',lr),\n",
    "                                   ('SVM',svc),\n",
    "                                   ('Decision Tree',dt),\n",
    "                                   ('Random Forest',rf),\n",
    "                                   ('AdaBoost',adb)],\n",
    "                      voting='soft',\n",
    "                      flatten_transform=True, \n",
    "                      weights=[1,5,2,4,3])\n",
    "vc2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation through Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, \n",
    "                             recall_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for LR: \n",
      " [[65  3]\n",
      " [ 8 24]]\n",
      "Accuracy for LR: \n",
      " 0.89\n",
      "Precision for LR: \n",
      " 0.8888888888888888\n",
      "Recall for LR: \n",
      " 0.75\n",
      "f1_score for LR: \n",
      " 0.8135593220338982\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for LR: \\n',confusion_matrix(y_test, lr.predict(X_test)))\n",
    "print('Accuracy for LR: \\n',accuracy_score(y_test, lr.predict(X_test)))\n",
    "acc.append(accuracy_score(y_test, lr.predict(X_test)))\n",
    "print('Precision for LR: \\n',precision_score(y_test, lr.predict(X_test)))\n",
    "precision.append(precision_score(y_test, lr.predict(X_test)))\n",
    "print('Recall for LR: \\n',recall_score(y_test, lr.predict(X_test)))\n",
    "recall.append(recall_score(y_test, lr.predict(X_test)))\n",
    "print('f1_score for LR: \\n',f1_score(y_test, lr.predict(X_test)))\n",
    "f1.append(f1_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for DTrees: \n",
      " [[62  6]\n",
      " [ 3 29]]\n",
      "Accuracy for DTrees: \n",
      " 0.91\n",
      "Precision for DTrees: \n",
      " 0.8285714285714286\n",
      "Recall for DTrees: \n",
      " 0.90625\n",
      "f1_score for DTrees: \n",
      " 0.8656716417910447\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for DTrees: \\n',confusion_matrix(y_test, dt.predict(X_test)))\n",
    "print('Accuracy for DTrees: \\n',accuracy_score(y_test, dt.predict(X_test)))\n",
    "acc.append(accuracy_score(y_test, dt.predict(X_test)))\n",
    "print('Precision for DTrees: \\n',precision_score(y_test, dt.predict(X_test)))\n",
    "precision.append(precision_score(y_test, dt.predict(X_test)))\n",
    "print('Recall for DTrees: \\n',recall_score(y_test, dt.predict(X_test)))\n",
    "recall.append(recall_score(y_test, dt.predict(X_test)))\n",
    "print('f1_score for DTrees: \\n',f1_score(y_test, dt.predict(X_test)))\n",
    "f1.append(f1_score(y_test, dt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SVM: \n",
      " [[64  4]\n",
      " [ 3 29]]\n",
      "Accuracy for SVM: \n",
      " 0.93\n",
      "Precision for SVM: \n",
      " 0.8787878787878788\n",
      "Recall for SVM: \n",
      " 0.90625\n",
      "f1_score for SVM: \n",
      " 0.8923076923076922\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for SVM: \\n',confusion_matrix(y_test, svc.predict(X_test)))\n",
    "print('Accuracy for SVM: \\n',accuracy_score(y_test, svc.predict(X_test)))\n",
    "acc.append(accuracy_score(y_test, svc.predict(X_test)))\n",
    "print('Precision for SVM: \\n',precision_score(y_test, svc.predict(X_test)))\n",
    "precision.append(precision_score(y_test, svc.predict(X_test)))\n",
    "print('Recall for SVM: \\n',recall_score(y_test, svc.predict(X_test)))\n",
    "recall.append(recall_score(y_test, svc.predict(X_test)))\n",
    "print('f1_score for SVM: \\n',f1_score(y_test, svc.predict(X_test)))\n",
    "f1.append(f1_score(y_test, svc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for RF: \n",
      " [[64  4]\n",
      " [ 4 28]]\n",
      "Accuracy for RF: \n",
      " 0.92\n",
      "Precision for RF: \n",
      " 0.875\n",
      "Recall for RF: \n",
      " 0.875\n",
      "f1_score for RF: \n",
      " 0.875\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for RF: \\n',confusion_matrix(y_test, rf.predict(X_test)))\n",
    "print('Accuracy for RF: \\n',accuracy_score(y_test, rf.predict(X_test)))\n",
    "acc.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "print('Precision for RF: \\n',precision_score(y_test, rf.predict(X_test)))\n",
    "precision.append(precision_score(y_test, rf.predict(X_test)))\n",
    "print('Recall for RF: \\n',recall_score(y_test, rf.predict(X_test)))\n",
    "recall.append(recall_score(y_test, rf.predict(X_test)))\n",
    "print('f1_score for RF: \\n',f1_score(y_test, rf.predict(X_test)))\n",
    "f1.append(f1_score(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for ADB: \n",
      " [[61  7]\n",
      " [ 3 29]]\n",
      "Accuracy for ADB: \n",
      " 0.9\n",
      "Precision for ADB: \n",
      " 0.8055555555555556\n",
      "Recall for ADB: \n",
      " 0.90625\n",
      "f1_score for ADB: \n",
      " 0.8529411764705882\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for ADB: \\n',confusion_matrix(y_test, adb.predict(X_test)))\n",
    "print('Accuracy for ADB: \\n',accuracy_score(y_test, adb.predict(X_test)))\n",
    "acc.append(accuracy_score(y_test, adb.predict(X_test)))\n",
    "print('Precision for ADB: \\n',precision_score(y_test, adb.predict(X_test)))\n",
    "precision.append(precision_score(y_test, adb.predict(X_test)))\n",
    "print('Recall for ADB: \\n',recall_score(y_test, adb.predict(X_test)))\n",
    "recall.append(recall_score(y_test, adb.predict(X_test)))\n",
    "print('f1_score for ADB: \\n',f1_score(y_test, adb.predict(X_test)))\n",
    "f1.append(f1_score(y_test, adb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for VC: \n",
      " [[62  6]\n",
      " [ 4 28]]\n",
      "Accuracy for VC: \n",
      " 0.9\n",
      "Precision for VC: \n",
      " 0.8235294117647058\n",
      "Recall for VC: \n",
      " 0.875\n",
      "f1_score for VC: \n",
      " 0.8484848484848485\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix for VC: \\n',confusion_matrix(y_test, vc.predict(X_test)))\n",
    "print('Accuracy for VC: \\n',accuracy_score(y_test, vc.predict(X_test)))\n",
    "acc.append(accuracy_score(y_test, vc.predict(X_test)))\n",
    "print('Precision for VC: \\n',precision_score(y_test, vc.predict(X_test)))\n",
    "precision.append(precision_score(y_test, vc.predict(X_test)))\n",
    "print('Recall for VC: \\n',recall_score(y_test, vc.predict(X_test)))\n",
    "recall.append(recall_score(y_test, vc.predict(X_test)))\n",
    "print('f1_score for VC: \\n',f1_score(y_test, vc.predict(X_test)))\n",
    "f1.append(f1_score(y_test, vc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'Models': models,\n",
    "             'Accuracies': acc,\n",
    "             'Precision': precision,\n",
    "             'Recall': recall,\n",
    "             'f1-score': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Accuracies</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.813559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Trees</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.892308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Average Ensemble</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.848485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models  Accuracies  Precision   Recall  f1-score\n",
       "0  Logistic Regression        0.89   0.888889  0.75000  0.813559\n",
       "1       Decision Trees        0.91   0.828571  0.90625  0.865672\n",
       "2                  SVM        0.93   0.878788  0.90625  0.892308\n",
       "3        Random Forest        0.92   0.875000  0.87500  0.875000\n",
       "4             AdaBoost        0.90   0.805556  0.90625  0.852941\n",
       "5     Average Ensemble        0.90   0.823529  0.87500  0.848485"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = pd.DataFrame(model_dict)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df.sort_values(['Accuracies', 'f1-score', 'Recall', 'Precision'],\n",
    "                               ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Accuracies</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.892308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Trees</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.90625</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Average Ensemble</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.87500</td>\n",
       "      <td>0.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.813559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models  Accuracies  Precision   Recall  f1-score\n",
       "2                  SVM        0.93   0.878788  0.90625  0.892308\n",
       "3        Random Forest        0.92   0.875000  0.87500  0.875000\n",
       "1       Decision Trees        0.91   0.828571  0.90625  0.865672\n",
       "4             AdaBoost        0.90   0.805556  0.90625  0.852941\n",
       "5     Average Ensemble        0.90   0.823529  0.87500  0.848485\n",
       "0  Logistic Regression        0.89   0.888889  0.75000  0.813559"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = lr, \n",
    "                             X = X_train, \n",
    "                             y = y_train, \n",
    "                             cv = 10)\n",
    "acMean = accuracies.mean()\n",
    "acStd = accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827081942899518"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09362431332005731"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80645161, 0.86666667, 0.73333333, 0.83333333, 0.7       ,\n",
       "       0.66666667, 0.86666667, 0.93333333, 0.93333333, 0.93103448])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------- GPU ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-347af773b203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                            \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                            n_jobs = -1)\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 667\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Applying Grid Search to find the best model and the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"n_estimators\": [100, 200, 300],\n",
    "              \"criterion\":['gini','entropy'],\n",
    "              \"max_depth\": [8, 16, 32],\n",
    "              \"min_samples_split\": [10, 20, 30],\n",
    "              \"min_samples_leaf\": [1, 5, 15],\n",
    "              \"min_weight_fraction_leaf\": [0.1, 0.05, 0.005]}\n",
    "grid_search = GridSearchCV(estimator = rf,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Final Model on training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "tunedRF = RandomForestClassifier(n_estimators = best_parameters[\"n_estimators\"],\n",
    "                                 criterion = best_parameters[\"criterion\"],\n",
    "                                 max_depth = best_parameters[\"max_depth\"],\n",
    "                                 min_samples_split = best_parameters[\"min_samples_split\"],\n",
    "                                 min_samples_leaf = best_parameters[\"min_samples_leaf\"],\n",
    "                                 min_weight_fraction_leaf = best_parameters[\"min_weight_fraction_leaf\"])\n",
    "tunedRF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion Matrix for Tuned RF: \\n',confusion_matrix(y_test, tunedRF.predict(X_test)))\n",
    "print('Accuracy for Tuned RF: \\n',accuracy_score(y_test, tunedRF.predict(X_test)))\n",
    "acc.append(accuracy_score(y_test, tunedRF.predict(X_test)))\n",
    "print('Precision for Tuned RF: \\n',precision_score(y_test, tunedRF.predict(X_test)))\n",
    "precision.append(precision_score(y_test, tunedRF.predict(X_test)))\n",
    "print('Recall for Tuned RF: \\n',recall_score(y_test, tunedRF.predict(X_test)))\n",
    "recall.append(recall_score(y_test, tunedRF.predict(X_test)))\n",
    "print('f1_score for Tuned RF: \\n',f1_score(y_test, tunedRF.predict(X_test)))\n",
    "f1.append(f1_score(y_test, tunedRF.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
